---
title: Celery异步任务平台
date: 2020-09-26 21:28:55
permalink: /pages/4bbab1/
categories:
  - 后端
  - 项目部署
tags:
  - 
---
# [Celery 快速入门](https://www.jianshu.com/p/7f7da1746ef2)

<p align="center">
  <img src="https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200928015648.png" width="500">
</p>

> 选择Redis作为broker


<!-- more -->

### redis安装
    yum install redis

## redis启动

    systemctl start redis.service   #启动redis服务
    systemctl stop redis.service   #停止redis服务
    systemctl restart redis.service   #重新启动服务
    systemctl status redis.service   #查看服务当前状态
    systemctl enable redis.service   #设置开机自启动
    systemctl disable redis.service   #停止开机自启动

### 基本用法

redis-cli进入redis服务

    # 进入本机redis
    redis-cli
    # 列出所有key
    keys *

默认配置文件（不用配置）

    vim /etc/redis.conf
    # 使配置生效
    redis-server /etc/redis.conf &

# python安装redis支持

    pip install redis

## 安装celery
    pip install -U "celery[redis]"
    
### 使用
创建Celery实例

```python
# tasks.py
from celery import Celery
app = Celery('tasks', broker='redis://localhost:6379/0')
```

创建任务

```python
# 假设这个发送邮件的任务需要5秒钟才能执行完

# tasks.py
@app.task
def send_mail(email):
    print("send mail to ", email)
    import time
    time.sleep(5)
    return "success"

# 在没有Celery的情况下，程序顺序执行，每个步骤都需要等上一步执行完成。又叫同步操作
# 1. 插入记录到数据库
# 2. 发邮件
# 3. 注册成功
```

启动worker

    # 启动 Worker，监听 Broker 中是否有任务，命令：celery worker，你可能需要指定参数
    
    celery -A tasks worker --loglevel=info
    
    # 修改了py任务代码需要重启服务，动态载入代码可能不用重启
    # -A： 指定 celery 实例在哪个模块中，例子中，celery实例在tasks.py文件中，启动成功后，能看到信息
    # 函数用app.task 装饰器修饰之后，就会成为Celery中的一个Task。
    
运行效果
![](https://upload-images.jianshu.io/upload_images/5395751-ad1cf3147210a35d.png?imageMogr2/auto-orient/strip|imageView2/2/w/849)

函数用app.task 装饰器修饰之后，就会成为Celery中的一个Task。

调用任务
```python
#在主程序中调用任务，将任务发送给 Broker， 而不是真正执行该任务，比如下面的主程序是 register

# user.py
from tasks import send_mail

def register():
    import time
    start = time.time()
    print("1. 插入记录到数据库")
    print("2. celery 帮我发邮件")
    send_mail.delay("xx@gmail.com")
    print("3. 告诉用户注册成功")
    print("耗时：%s 秒 " % (time.time() - start))

if __name__ == '__main__':
    register()

# 在主程序中，调用函数的.delay方法
```

目录结构：

    ── celery_test
       ├── tasks.py
       └── user.py

# 注意：

celery worker 启动时，如果是root用户，需要设置环境变量：
> 不一定要设置

    $ export C_FORCE_ROOT='true'

不要使用复杂对象作为任务函数的参数
```python
# Good
@app.task
def my_task(user_id):
    user = User.objects.get(id=user_id)
    print(user.name)
    # ...
# Bad
@app.task
def my_task(user):
    print(user.name)
    # ...
```

# 其他操作：

[分布式队列神器 Celery](https://segmentfault.com/a/1190000008022050)

进度监控

```python
# tasks.py
from celery import Celery
import time
app = Celery('tasks', broker='redis://localhost:6379/0', backend ='redis://127.0.0.1:6379/0')

@app.task(bind=True)
def test_mes(self):
    for i in range(1, 11):
        time.sleep(1)
        self.update_state(state="PROGRESS", meta={'p': i*20})    # 更新当前状态信息
    return 'finish'
```

监控代码
```python
from tasks import test_mes
from tasks import app
import sys
from celery.result import AsyncResult
import time

def pm():
    job = test_mes.delay()
    job_id = job.id # 返回给用户
    # 过5秒在来看状态
    time.sleep(5)
    print('突然想查看任务进度')
    job = AsyncResult(job_id)
    while job.status in ['PENDING', 'PROGRESS']:
        time.sleep(1)
        print("res:", job.result)

if __name__ == '__main__':
    # register()
    pm()

```

配合BackTrader报错

    AssertionError: daemonic processes are not allowed to have children

解决办法
> 第一点就解决了

    1.在终端设置环境变量启用优化模式，export PYTHONOPTIMIZE=1，    
    2.再执行celery -A app.celery.base worker -l info -n socwebai就行了
    3.如果用的multiprocessing，重写一个Mypool：https://stackoverflow.com/questions/6974695/python-process-pool-non-daemonic（没试）

