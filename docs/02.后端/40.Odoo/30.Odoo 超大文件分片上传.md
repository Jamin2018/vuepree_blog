---
title: Odoo 超大文件分片上传
date: 2020-09-29 09:28:38
permalink: /pages/5f3c97/
categories:
  - 后端
  - Odoo
tags:
  - 
---
# Odoo 超大文件分片上传

## 起因

### 使用Muk模块的时候，在上传超过500mb 文件的时候，服务器报错，报错提示`MemoryError`



<img src="https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200929232624.png" alt="image-20200928211904767" style="zoom:50%;" />

> 解决方案：Odoo.conf文件限制了服务器内存大小，这里在最后加了一个0，提升至10倍用于测试。

<!-- more -->

![image-20200928210119467](https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200929233151.png)

### 继续上传超过500mb 文件的时候，没报错，但是请求页面开始无响应，odoo服务开始重启。

>解决方案：由于我是使用docker环境跑的，容器默认分配2g大小，使用`docker stats`查看对应的容器资源使用情况，发现在上传500 mb的文件的时候，会先使用内核缓存进行文件存储，然后才进行文件读写操作，这样一来一回，加上服务器代码的对大文件对象的各种操作，导致使用内存基本是上传的文件大小的5倍。所以我用Docker Desktop 工具将虚拟机内存调大至6g，用于测试大文件上传。

<img src="https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200929233229.png" alt="image-20200928211243627" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200929233306.png" alt="image-20200928210447933" style="zoom:50%;" />



### 上传500mb 使服务器的问题解决了，又发现前端的在选择按钮选择大于800MB的文件的时候，浏览器调试会提示 data数据为null

<img src="https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200929233335.png" alt="image-20200928211018401" style="zoom:50%;" />

经排查， 得知前端传输数据有大小限制，超过一定大小则

<img src="https://cdn.jsdelivr.net/gh/Jamin2018/static_file/img/20200929233343.png" alt="image-20200929130302689" style="zoom:50%;" />

并且在odoo的js机制FieldBinaryFile字段，会使用WEB API的`FileReader`方法对文件进行操作，`FileReader`没法读取超过一定大小的文件的时候，虽然读取文件失败，但是还执行回调函数`filereader.onloadend`，其中的`var data = upload.target.result;`没有值

```javascript
var file = file_node.files[0];
                if (file.size > this.max_upload_size) {
                    var msg = _t("The selected file exceed the maximum file size of %s.");
                    this.do_warn(_t("File upload"), _.str.sprintf(msg, utils.human_size(this.max_upload_size)));
                    return false;
                }
                var filereader = new FileReader();
                filereader.readAsDataURL(file);
                filereader.onloadend = function (upload) {
                    var data = upload.target.result;
                    data = data.split(',')[1];
                    self.on_file_uploaded(file.size, file.name, file.type, data);
                };
```

那有没有一个办法解决这个问题呢？接下来就涉及到本文的主题：超大文件分片上传

[Python实现大文件分片上传](https://blog.csdn.net/jinixin/article/details/77545140)

1. 定义前后端对接的数据结构

   ```
   json  = {
               res_id: 'file记录id，用这个来找存储路径',
               chunk: '当前属于第几片',   # task+chunk变成临时储存文件，用于合并的时候按顺序
               file_type: '数据类型',
               file_name: '文件名称',	# 用于后端合并文件的时候的名字
               file_size: '总大小',
               content: '数据内容',
               start: '开始的字节位',
               end: '结束的字节位',
   
               total_pieces: '总共多少片'
           }
   ```

   

2. 利用FileReader和file.slice的切片功能，自定义Odoo widget

   自定义widget 继承 FieldBinaryFile，利用了该组件的

```javascript
	    odoo.define('muk_dms_large_widgets.binary', function (require) {
    "use strict";

    var core = require('web.core');
    var registry = require('web.field_registry');
    var field_utils = require('web.field_utils');
    var field_widgets = require('web.basic_fields');
    var Session = require('web.session');
    var _t = core._t;
    var QWeb = core.qweb;
    var framework = require('web.framework');
    var crash_manager = require('web.crash_manager');
    var utils = require('web.utils');

    var MyFieldDocumentBinary = field_widgets.FieldBinaryFile.extend({
        init: function () {
            this._super.apply(this, arguments);
            this.filename_value = this.recordData[this.attrs.filename];
            this.big_file = [];
        },
        willStart: function () {
            var self = this;
            var load_config = this._rpc({
                route: '/config/muk_dms.max_upload_size',
            }).done(function (result) {
                self.max_upload_size = result.max_upload_size * 1024 * 1024;
            });
            return $.when(this._super.apply(this, arguments), load_config);
        },


        on_file_change: function (e) {

            var self = this;
            var chunkSize = 1024 * 1024 * 300;
            var file_node = e.target;
            if ((this.useFileAPI && file_node.files.length) || (!this.useFileAPI && $(file_node).val() !== '')) {
                if (this.useFileAPI) {
                    var file = file_node.files[0];
                    if (file.size > this.max_upload_size) {
                        var msg = _t("The selected file exceed the maximum file size of %s.");
                        this.do_warn(_t("File upload"), _.str.sprintf(msg, utils.human_size(this.max_upload_size)));
                        return false;
                    }

                    // self.record.res_id有则说明保存了文件
                    if (self.record.res_id) {
                        var chunks = Math.ceil(file.size / chunkSize);
                        var filder = new Array(chunks);
                        var big_file_list = new Array(chunks);
                        var uploaded = [];
                        var start = 0, end = 0;


                        console.log("文件总尺寸" + file.size);


                        for (let curindex = 0; curindex < chunks; curindex++) {

                            if (file.size - start <= chunkSize) {
                                end = file.size;
                            } else {
                                end = start + chunkSize;
                            }
                            console.log("本次切割范围:" + curindex + " " + start + " " + end);

                            big_file_list[curindex] = {
                                data: "",
                                file_type: file.type,
                                file_name: file.name,
                                chunk: curindex,
                                start: start,
                                end: end,
                                res_id: self.record.res_id,

                            };
                            filder[curindex] = new FileReader();
                            filder[curindex].readAsDataURL(file.slice(start, end));
                            filder[curindex].onload = function () {

                                big_file_list[curindex].data = filder[curindex].result.split(',')[1];

                                Session.rpc('/dms/upload_to_server', {
                                    data: big_file_list[curindex],
                                }).then(function (result) {
                                    console.log(curindex, '#', result)
                                    uploaded.push(curindex);

                                    // 并判断是不是最后一片返回成功
                                    if (uploaded.length === chunks) {
                                        console.log('最后一片加载完毕，上传接受完后');
                                        // 触发新接口，合并数据

                                        Session.rpc('/dms/upload_to_server_merge', {
                                            data: big_file_list[curindex],
                                        }).then(function (result) {
                                            console.log('文件合并成功')
                                        });

                                    }
                                });


                            };
                            start = end;
                        }

                        self.set_filename(file.name);
                        this._render();
                    } else {
                        var filereader = new FileReader();
                        filereader.readAsDataURL(file);
                        filereader.onloadend = function (upload) {
                            var data = upload.target.result;
                            data = data.split(',')[1];
                            self.on_file_uploaded(file.size, file.name, file.type, data);
                        };
                    }

                } else {
                    this.$('form.o_form_binary_form input[name=session_id]').val(this.getSession().session_id);
                    this.$('form.o_form_binary_form').submit();
                }
                this.$('.o_form_binary_progress').show();
                this.$('button').hide();
            }
        },


        on_save_as: function (ev) {
            console.log('下载时候触发');
            console.log(this.record)
            console.log(this.res_id , this.record.is_large_file)
            if (!this.value) {
                this.do_warn(_t("Save As..."), _t("The field is empty, there's nothing to save !"));
                ev.stopPropagation();
            } else if (this.res_id && !this.record.is_large_file) {
                framework.blockUI();
                var c = crash_manager;
                var filename_fieldname = this.attrs.filename;
                this.getSession().get_file({
                    'url': '/web/content',
                    'data': {
                        'model': this.model,
                        'id': this.res_id,
                        'field': this.name,
                        'filename_field': filename_fieldname,
                        'filename': this.recordData[filename_fieldname] || null,
                        'download': true,
                        'data': utils.is_bin_size(this.value) ? null : this.value,
                    },
                    'complete': framework.unblockUI,
                    'error': c.rpc_error.bind(c),
                });
                ev.stopPropagation();
            }else if(this.res_id && this.record.is_large_file){
                console.log('触发大文件下载');

                framework.blockUI();
                var c = crash_manager;
                var filename_fieldname = this.attrs.filename;
                this.getSession().get_file({
                    'url': '/web/content',
                    'data': {
                        'model': this.model,
                        'id': this.res_id,
                        'field': this.name,
                        'filename_field': filename_fieldname,
                        'filename': this.recordData[filename_fieldname] || null,
                        'download': true,
                        'data': utils.is_bin_size(this.value) ? null : this.value,
                    },
                    'complete': framework.unblockUI,
                    'error': c.rpc_error.bind(c),
                });
                ev.stopPropagation();
            }
        },

    });

    var MyFieldBoolean = field_widgets.FieldBoolean.extend({
        _onChange: function () {
            this._setValue(this.$input[0].checked);
        },

    });

    registry.add('my_dms_binary', MyFieldDocumentBinary);
    registry.add('my_boolean', MyFieldBoolean);

    return {MyFieldDocumentBinary};

});

```



2.继承修改上传文件的字段widget

```xml
<odoo>
        <record model="ir.ui.view" id="view_dms_file_form_inherit">
        <field name="name">muk_dms_file_.form.inherit</field>
        <field name="model">muk_dms.file</field>
        <field name="inherit_id" ref="muk_dms.view_dms_file_form"/>
        <field name="arch" type="xml">
            <xpath expr="//field[@filename='name']" position="replace">
                    <field name="content" filename="name" widget="my_dms_binary"/>
            </xpath>
            <xpath expr="//field[@filename='name']" position="after">
                    <field name="is_large_file" widget="my_boolean"/>
            </xpath>
        </field>
    </record>
</odoo>
```

3.新增controllers Url用于切片接受和合并数据

```python
import os
import base64
import logging

import werkzeug.utils
import werkzeug.wrappers

from odoo import http
from odoo.http import request
from odoo.exceptions import AccessError

_logger = logging.getLogger(__name__)


class MyDocumentController(http.Controller):

    @http.route('/dms/upload_to_server', type='json', auth="user")
    def upload(self, data, **kw):
        # record = request.env['muk_dms.directory'].browse([id])
        _logger.info('--' * 100)
        file_obj = request.env['muk_dms.file'].browse(data['res_id'])
        file_path = file_obj.reference._build_path()
        _logger.info(file_path)
        _logger.info('/'.join(file_path.split('/')[:-1]))
        file_path = '/'.join(file_path.split('/')[:-1])
        task = file_obj.reference.checksum + str(file_obj.reference.id)        # 获取文件唯一标识符
        chunk = data['chunk']     # 获取该分片在所有分片中的序号
        filename = '%s%s' % (task, chunk)           # 构成该分片唯一标识符

        file_path = file_path +'/temp/' + filename

        if not os.path.exists(os.path.dirname(file_path)):
            os.makedirs(os.path.dirname(file_path))

        with open(file_path, "wb+") as file_handler:
            file = base64.b64decode(bytes(data['data'], encoding="utf8"))
            file_handler.write(file)  # 读取分片内容写入新文件

        return werkzeug.wrappers.Response(status=200)


    @http.route('/dms/upload_to_server_merge', type='json', auth="user")
    def upload_merge(self, data, **kw):

        _logger.info('===' * 100)
        file_obj = request.env['muk_dms.file'].browse(data['res_id'])
        file_path = file_obj.reference._build_path()
        file_path = '/'.join(file_path.split('/')[:-1])


        target_filename = data['file_name']  # 获取上传文件的文件名
        task = file_obj.reference.checksum + str(file_obj.reference.id)   # 获取文件的唯一标识符
        chunk = 0  # 分片序号
        with open('%s/%s' % (file_path, target_filename), 'wb') as target_file:  # 创建新文件
            while True:
                try:
                    filename = '%s/temp/%s%d' % (file_path, task, chunk)
                    source_file = open(filename, 'rb')  # 按序打开每个分片
                    target_file.write(source_file.read())  # 读取分片内容写入新文件
                    source_file.close()
                except IOError:
                    break
                chunk += 1
                os.remove(filename)  # 删除该分片，节约空间

        return werkzeug.wrappers.Response(status=200)


```

## 总结

Odoo前端有自己一套js机制，在前端组件之间实现数据传递没有Vue那么简单，并且教程也几乎没有，好在最近系统的学习了Vue，Vue的学习让我对Odoo的js有种触类旁通的感觉。



